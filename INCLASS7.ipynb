{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKSTLF2BX6jH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N11Ee3GJmywu",
        "outputId": "64e10b54-dad3-48e2-c266-617670a17e82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/26/a1/75474477af2a1dae3a25f80b72bbaf20e8296191ece7fff2f67984206f33/openai-1.12.0-py3-none-any.whl.metadata\n",
            "  Downloading openai-1.12.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\sulta\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
            "Collecting distro<2,>=1.7.0 (from openai)\n",
            "  Obtaining dependency information for distro<2,>=1.7.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\n",
            "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/41/7b/ddacf6dcebb42466abd03f368782142baa82e08fc0c1f8eaa05b4bae87d5/httpx-0.27.0-py3-none-any.whl.metadata\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\sulta\\anaconda3\\lib\\site-packages (from openai) (1.10.8)\n",
            "Requirement already satisfied: sniffio in c:\\users\\sulta\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\sulta\\anaconda3\\lib\\site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\sulta\\anaconda3\\lib\\site-packages (from openai) (4.7.1)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\sulta\\anaconda3\\lib\\site-packages (from wikipedia) (4.12.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\sulta\\anaconda3\\lib\\site-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\sulta\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: certifi in c:\\users\\sulta\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/2c/93/13f25f2f78646bab97aee7680821e30bd85b2ff0fc45d5fdf5393b79716d/httpcore-1.0.4-py3-none-any.whl.metadata\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Obtaining dependency information for h11<0.15,>=0.13 from https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sulta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sulta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.16)\n",
            "Requirement already satisfied: colorama in c:\\users\\sulta\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sulta\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.4)\n",
            "Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "   ---------------------------------------- 0.0/226.7 kB ? eta -:--:--\n",
            "   ------------ --------------------------- 71.7/226.7 kB 3.8 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 153.6/226.7 kB 2.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 226.7/226.7 kB 2.3 MB/s eta 0:00:00\n",
            "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "   ---------------------------------------- 0.0/75.6 kB ? eta -:--:--\n",
            "   ---------------------------------------- 75.6/75.6 kB 4.1 MB/s eta 0:00:00\n",
            "Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "   ---------------------------------------- 0.0/77.8 kB ? eta -:--:--\n",
            "   ---------------------------------------- 77.8/77.8 kB 4.2 MB/s eta 0:00:00\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
            "   ----------------------------------- ---- 51.2/58.3 kB 2.6 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 51.2/58.3 kB 2.6 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 51.2/58.3 kB 2.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 58.3/58.3 kB 384.2 kB/s eta 0:00:00\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py): started\n",
            "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11707 sha256=51ede0688447a032466e799b1944d4cb95b9fb935958f9cc8a46e7a31c5eb628\n",
            "  Stored in directory: c:\\users\\sulta\\appdata\\local\\pip\\cache\\wheels\\8f\\ab\\cb\\45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: h11, distro, wikipedia, httpcore, httpx, openai\n",
            "Successfully installed distro-1.9.0 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0 wikipedia-1.4.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install openai wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q2A8TGhKm3i5"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "import wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E9HEMJSX-3T"
      },
      "source": [
        "# 1.) Set up OpenAI and the enviornment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4zwwdkZDYDZN"
      },
      "outputs": [],
      "source": [
        "apikey = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8IiKS0snlpYP"
      },
      "outputs": [],
      "source": [
        "openai.api_key = apikey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = openai.OpenAI(api_key = openai.api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOXc5_BTm9HP"
      },
      "source": [
        "# 2.) Use the wikipedia api to get a function that pulls in the text of a wikipedia page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "page_titles = [\"Artificial Intelligence\", \"UCLA\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "page_title = page_titles[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "search_results = wikipedia.search(page_title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-v7OYamHlrEB"
      },
      "outputs": [],
      "source": [
        "page = wikipedia.page(search_results[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TgY2FkTdmhTH"
      },
      "outputs": [],
      "source": [
        "def get_wikipedia_content(page_title):\n",
        "    search_results = wikipedia.search(page_title)\n",
        "    page = wikipedia.page(search_results[0])\n",
        "    return(page.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Kw5H5jMlmmS3"
      },
      "outputs": [],
      "source": [
        "content = get_wikipedia_content(page_title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF3BiZyXltYO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef7yfa2jl0iZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9aruncMmubX"
      },
      "source": [
        "# 3.) Build a chatgpt bot that will analyze the text given and try to locate any false info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Bmai3B6Dmw3O"
      },
      "outputs": [],
      "source": [
        "chat_completions = client.chat.completions.create(\n",
        "    model = \"gpt-4\",\n",
        "    messages = [\n",
        "        {\"role\" : \"system\", \"content\" : \" I will be giving you an article. I am looking for false information. I want to capture all potentially false info, if there is even small potential for it to be wrong, please return it. Please consicely list only the Only give me information that is false from this article. Do not give me a summary.\"},\n",
        "        {\"role\" : \"user\", \"content\" : content[:8180]}]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1jI-un5PnDjg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. \"Its academic roots were established in 1881 as a normal school then known as the southern branch of the California State Normal School which later evolved into San José State University.\" - UCLA is a separate entity and did not evolve into San José State University.\n",
            "2. \"The branch was transferred to the University of California, becoming the Southern Branch of UC in 1919, making it the second-oldest of the ten-campus University of California system after the University of California, Berkeley.\" - The fact about UCLA being the second-oldest UC campus could be potentially inaccurate, since UCSF was established in 1864 and UC Davis in 1905.\n",
            "3. \"The university is organized into the College of Letters and Science and twelve professional schools.\" - There are seven professional schools in UCLA, not twelve.\n",
            "4. \"They have won 121 NCAA team championships, second only to Stanford University's 128 team titles.\" - UCLA currently holds 118 NCAA team championships, Stanford's count is also incorrect.\n",
            "5. \"410 Bruins have made Olympic teams, winning 270 Olympic medals: 136 gold, 71 silver and 63 bronze.\" - The count of Bruins that made it to Olympic teams and the total medals won may be outdated or inaccurate.\n",
            "6. \"As of October 2021, 27 Nobel laureates, five Turing Award winners, two Chief Scientists of the U.S. Air Force and one Fields Medalist have been affiliated with it as faculty, researchers and alumni.\" - The numbers for the Nobel laureates, Turing Award winners, Chief Scientists of the U.S. Air Force and Fields Medalist might not be accurate.\n",
            "7. \"The Regents announced the new \"Beverly Site\" — just west of Beverly Hills — in 1925.\" - The new site is known as Westwood and not the Beverly site.\n",
            "8. \"The campus in Westwood opened to students in 1929.\" - The UCLA campus opened to students in 1930, not 1929.\n",
            "9. \"In late 1923, the Board of Regents authorized a fourth year of instruction and transformed the Junior College into the College of Letters and Science, which awarded its first bachelor's degrees in 1925.\" - The first bachelor's degree were awarded in 1927, not 1925.\n",
            "10. \"During its first 32 years, UCLA was treated as an off-site department of the main campus in Berkeley.\" - This could be potentially inaccurate as UCLA became independent rather quickly and was not considered a department of Berkley.\n",
            "11. \"In 2018, a student-led community coalition known as \"Westwood Forward\" successfully led an effort to break UCLA and Westwood Village away from the existing Westwood Neighborhood Council and form a new North Westwood Neighborhood Council, with over 2,000 out of 3,521 stakeholders voting in favor of the split.\" - The accuracy of the details regarding the formation of the new North Westwood Neighborhood Council and the number of stakeholders is uncertain.\n",
            "12. \"In 2022, UCLA signed an agreement to partner with the Tongva for the caretaking and landscaping of various areas of the campus. This included land use for ceremonial events and educational workshops and outreach events.\" - This could potentially be inaccurate as the agreement could be scheduled for a future date or it may not have been signed at all.\n"
          ]
        }
      ],
      "source": [
        "print(chat_completions.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_TMKFGN4nDJ4"
      },
      "outputs": [],
      "source": [
        "def chatgpt_error_correction(text):\n",
        "    chat_completions = client.chat.completions.create(\n",
        "    model = \"gpt-4\",\n",
        "    messages = [\n",
        "        {\"role\" : \"system\", \"content\" : \"Only give me information that is false from this article. Do not give me a summary.\"},\n",
        "        {\"role\" : \"user\", \"content\" : content[:8180]}]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FKAJVXSoayA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPw5LyPEobmk"
      },
      "source": [
        "# 4.) Make a for loop and check a few wikipedia pages and return a report of any potentially false info via wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "V7cuhML2ocGn"
      },
      "outputs": [],
      "source": [
        "page_titles = [\"Artificial Intelligence\", \"UCLA\", \"Rain\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_______Artificial Intelligence\n",
            "_______UCLA\n",
            "_______Rain\n"
          ]
        }
      ],
      "source": [
        "for page_title in page_titles:\n",
        "    try:\n",
        "        print(\"_______\" + page_title)\n",
        "        content = get_wikipedia_content(page_title)\n",
        "        chatgpt_error_correction(content)\n",
        "    except:\n",
        "        print(\"ERROR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
